{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcf4b092-1605-4115-90e4-36d0fab4517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "# import matplotlib.pyplot as plt\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "#from wordcloud import WordCloud\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b75812cf-23a6-48d6-a474-897d7725c2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\KIIT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\KIIT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')\n",
    "# Load positive and negative tweets\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e692a-e9f4-4863-8a03-0c47bc0ccb31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f84cd35a-aa3b-4327-9331-ff473e9b0b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "    'tweet': positive_tweets + negative_tweets,\n",
    "    'label': [1] * len(positive_tweets) + [0] * len(negative_tweets)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "236ac21d-868a-45e6-8928-4900eaf8cc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Tweets:\n",
      "                                                tweet  label\n",
      "0  #FollowFriday @France_Inte @PKuchly57 @Milipol...      1\n",
      "1  @Lamb2ja Hey James! How odd :/ Please call our...      1\n",
      "2  @DespiteOfficial we had a listen last night :)...      1\n",
      "3                               @97sides CONGRATS :)      1\n",
      "4  yeaaaah yippppy!!!  my accnt verified rqst has...      1\n",
      "\n",
      "Label Distribution:\n",
      " label\n",
      "1    5000\n",
      "0    5000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSample Tweets:\\n\", data.head())\n",
    "print(\"\\nLabel Distribution:\\n\", data['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cb3fa1-604a-42e2-854a-d300a01bc6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a986333b-0a3b-4701-9666-58a40114e990",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_cleaned = [preprocess_tweet(tweet) for tweet in positive_tweets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02f8cae4-350a-4a16-9dbe-4dee4d05de68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "651f5d73-b7f0-4ca7-af8f-aac979a8b6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean tweets\n",
    "def preprocess_tweet(tweet):\n",
    "# Remove URLs and mentions\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|@\\S+', '', tweet)  \n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    clean_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and word.isalpha()]\n",
    "    return ' '.join(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "989471b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\KIIT\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d240802-baa3-4ce4-a524-909f56a40f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_cleaned = [preprocess_tweet(tweet) for tweet in negative_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "189a5702-c696-48ca-b315-77dccae14de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Tweets Sample:\n",
      "                                                tweet  label\n",
      "0                  top engaged member community week      1\n",
      "1  hey james odd please call contact centre able ...      1\n",
      "2     listen last night bleed amazing track scotland      1\n",
      "3                                           congrats      1\n",
      "4  yeaaah yipppy accnt verified rqst succeed got ...      1\n"
     ]
    }
   ],
   "source": [
    "# Create cleaned dataset\n",
    "cleaned_data = pd.DataFrame({\n",
    "    'tweet': positive_cleaned + negative_cleaned,\n",
    "    'label': [1] * len(positive_cleaned) + [0] * len(negative_cleaned)\n",
    "})\n",
    "\n",
    "print(\"\\nCleaned Tweets Sample:\\n\", cleaned_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "352fcc57-e1dc-4d60-9683-0af582b7dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction: TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(cleaned_data['tweet']).toarray()\n",
    "y = cleaned_data['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52285a72-0167-4178-8560-442b9dd282d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e044fcae-4e71-4ce6-835a-4a05be8f1160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Accuracy: 0.745\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.metrics import  accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "print(\"\\nLogistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c0c622d-c03f-46e1-9f40-1caff2981b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random Forest Classifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# rf_model = RandomForestClassifier(random_state=42)\n",
    "# rf_model.fit(X_train, y_train)\n",
    "# y_pred_rf = rf_model.predict(X_test)\n",
    "# print(\"\\nRandom Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dc5e76b-e65d-4aa0-b4f6-0ce9761c0599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Support Vector Machine\n",
    "# from sklearn.svm import SVC\n",
    "# svm_model = SVC(kernel='linear', random_state=42)\n",
    "# svm_model.fit(X_train, y_train)\n",
    "# y_pred_svm = svm_model.predict(X_test)\n",
    "# print(\"\\nSupport Vector Machine Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d80d5823-ca39-4802-9cac-cddfeabebaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict user input sentiment\n",
    "def predict_sentiment(input_text, model, vectorizer):\n",
    "    cleaned_text = preprocess_tweet(input_text)\n",
    "    text_features = vectorizer.transform([cleaned_text])\n",
    "    prediction = model.predict(text_features)[0]\n",
    "    return \"Positive\" if prediction == 1 else \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9ba9a7-98df-4b3d-9616-593797e897a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe8b749d-066a-4ede-a2c5-b0d5be9c4c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the text  hii we are good \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "# User input for prediction\n",
    "user_input = input(\"enter the text \")\n",
    "cleaned_input = preprocess_tweet(user_input)\n",
    "input_features = vectorizer.transform([cleaned_input])\n",
    "predicted_sentiment = lr_model.predict(input_features)[0]\n",
    "result = \"Positive\" if predicted_sentiment == 1 else \"Negative\"\n",
    "print(f\"Predicted Sentiment: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3d31563-023e-4711-b217-06a5bec180c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the text  I’m so disappointed with the results. Never buying again.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "# User input for prediction\n",
    "user_input = input(\"enter the text \")\n",
    "cleaned_input = preprocess_tweet(user_input)\n",
    "input_features = vectorizer.transform([cleaned_input])\n",
    "predicted_sentiment = lr_model.predict(input_features)[0]\n",
    "result = \"Positive\" if predicted_sentiment == 1 else \"Negative\"\n",
    "print(f\"Predicted Sentiment: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8db55ce4-0870-4fc1-97f9-9522c1de8319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the text  Great work by the team! Looking forward to more success\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "# User input for prediction\n",
    "user_input = input(\"enter the text \")\n",
    "cleaned_input = preprocess_tweet(user_input)\n",
    "input_features = vectorizer.transform([cleaned_input])\n",
    "predicted_sentiment = lr_model.predict(input_features)[0]\n",
    "result = \"Positive\" if predicted_sentiment == 1 else \"Negative\"\n",
    "print(f\"Predicted Sentiment: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b47ab521-15cb-4210-9a82-ff470f13b8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the text  good game\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "# User input for prediction\n",
    "user_input = input(\"enter the text \")\n",
    "cleaned_input = preprocess_tweet(user_input)\n",
    "input_features = vectorizer.transform([cleaned_input])\n",
    "predicted_sentiment = lr_model.predict(input_features)[0]\n",
    "result = \"Positive\" if predicted_sentiment == 1 else \"Negative\"\n",
    "print(f\"Predicted Sentiment: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "247d5e5b-aae0-4eb9-a97b-4355d6d8fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "# MongoDB Connection\n",
    "client = MongoClient(\"mongodb+srv://ktaunk28:qwerty123@cluster0.dim29.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\")  # Local MongoDB instance\n",
    "db = client[\"journaldb\"]  # Replace 'sentiment_db' with your database name\n",
    "collection = db[\"users\"]  # Replace 'input_tweets' with your collection name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "182dadc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('6767eeeef8ebd73743f041fc'), 'title': 'This is my first day', 'date': datetime.datetime(2024, 12, 22, 10, 50, 22, 652000), 'content': 'The situation at border seems to be critical and we could expect a world war3 ', 'sentiment': '', '_class': 'net.khushtaunk.journalApp.Entity.journalEntry'}\n",
      "{'_id': ObjectId('6767f076f8ebd73743f041fd'), 'title': 'This is my second day', 'date': datetime.datetime(2024, 12, 22, 10, 56, 54, 327000), 'content': 'Good game ', 'sentiment': '', '_class': 'net.khushtaunk.journalApp.Entity.journalEntry'}\n",
      "{'_id': ObjectId('6767f092f8ebd73743f041fe'), 'title': 'This is my third day', 'date': datetime.datetime(2024, 12, 22, 10, 57, 22, 531000), 'content': 'I am so disappointed with the result,never buying it again', 'sentiment': '', '_class': 'net.khushtaunk.journalApp.Entity.journalEntry'}\n",
      "{'_id': ObjectId('6767f0b8f8ebd73743f041ff'), 'title': 'This is my fourth day', 'date': datetime.datetime(2024, 12, 22, 10, 58, 0, 475000), 'content': 'Great day it was, no terrorist acitivity', 'sentiment': '', '_class': 'net.khushtaunk.journalApp.Entity.journalEntry'}\n",
      "Sentiment analysis results saved to MongoDB.\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from bson.objectid import ObjectId\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient(\"mongodb+srv://ktaunk28:qwerty123@cluster0.dim29.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\")  # Replace with your MongoDB URI if needed\n",
    "db = client['journaldb']  # Connect to the journaldb database\n",
    "users_collection = db['users']  # Access the users collection\n",
    "journal_entries_collection = db['journal_Entries']  # Access the journal_Entries collection\n",
    "\n",
    "# Fetch a specific user by ID\n",
    "user_id = ObjectId(\"6767bb0e0e081605587affee\")  # Replace with the actual user ObjectId\n",
    "user_document = users_collection.find_one({\"_id\": user_id})\n",
    "\n",
    "# Ensure user exists and has journal entries\n",
    "if user_document and \"journalEntries\" in user_document:\n",
    "    journal_entries_refs = user_document[\"journalEntries\"]  # Array of DBRef objects\n",
    "\n",
    "    # Fetch the journal entries from the referenced collection\n",
    "    journal_entries = []\n",
    "    for ref in journal_entries_refs:\n",
    "        entry_id = ref.id  # Get the ObjectId of the journal entry\n",
    "        entry_document = journal_entries_collection.find_one({\"_id\": entry_id})\n",
    "        if entry_document:\n",
    "            journal_entries.append(entry_document)\n",
    "\n",
    "    # Example: Display fetched journal entries\n",
    "    for entry in journal_entries:\n",
    "        print(entry)\n",
    "\n",
    "    # Process and analyze each journal entry\n",
    "    def analyze_journal_entry(entry_text):\n",
    "        cleaned_entry = preprocess_tweet(entry_text)  # Assuming a preprocess_tweet() function\n",
    "        features = vectorizer.transform([cleaned_entry])  # Vectorize the text\n",
    "        prediction = lr_model.predict(features)[0]  # Predict sentiment\n",
    "        return \"Positive\" if prediction == 1 else \"Negative\"\n",
    "\n",
    "    # Perform sentiment analysis\n",
    "    analysis_results = []\n",
    "    for entry in journal_entries:\n",
    "        if \"text\" in entry:  # Assuming each journal entry has a \"text\" field\n",
    "            sentiment = analyze_journal_entry(entry[\"text\"])\n",
    "            analysis_results.append({\"entry_id\": entry[\"_id\"], \"sentiment\": sentiment})\n",
    "\n",
    "    # Save the analysis results back to the user's document\n",
    "    users_collection.update_one(\n",
    "        {\"_id\": user_id},\n",
    "        {\"$set\": {\"journal_Analysis\": analysis_results}}  # Add a new field for analysis results\n",
    "    )\n",
    "    print(\"Sentiment analysis results saved to MongoDB.\")\n",
    "else:\n",
    "    print(\"User not found or no journal entries available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0fa4653d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Results: [{'entry_id': ObjectId('6767eeeef8ebd73743f041fc'), 'content': 'The situation at border seems to be critical and we could expect a world war3 ', 'sentiment': 'Negative'}, {'entry_id': ObjectId('6767f076f8ebd73743f041fd'), 'content': 'Good game ', 'sentiment': 'Positive'}, {'entry_id': ObjectId('6767f092f8ebd73743f041fe'), 'content': 'I am so disappointed with the result,never buying it again', 'sentiment': 'Negative'}, {'entry_id': ObjectId('6767f0b8f8ebd73743f041ff'), 'content': 'Great day it was, no terrorist acitivity', 'sentiment': 'Positive'}]\n"
     ]
    }
   ],
   "source": [
    "# Apply analysis to the content field of all journal entries\n",
    "results = []\n",
    "\n",
    "# Loop through journal entries and analyze their 'content' fields\n",
    "for entry in journal_entries:\n",
    "    if \"content\" in entry:  # Ensure the 'content' field exists\n",
    "        sentiment = analyze_journal_entry(entry[\"content\"])  # Pass only the 'content' field\n",
    "        results.append({\n",
    "            \"entry_id\": entry[\"_id\"],  # Include the unique ID of the journal entry\n",
    "            \"content\": entry[\"content\"],  # Include the original content for reference\n",
    "            \"sentiment\": sentiment       # Include the sentiment analysis result\n",
    "        })\n",
    "    else:\n",
    "        print(f\"Entry with ID {entry['_id']} has no 'content' field.\")  # Log missing 'content' fields\n",
    "\n",
    "print(\"Analysis Results:\", results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61be0e81",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f811c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the journal entries with sentiment analysis results\n",
    "for result in results:\n",
    "    db[\"journal_Entries\"].update_one(\n",
    "        {\"_id\": result[\"entry_id\"]},\n",
    "        {\"$set\": {\"sentiment\": result[\"sentiment\"]}}\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
